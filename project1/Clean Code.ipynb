{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " # Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_file):\n",
    "    \"\"\"load data.\"\"\"\n",
    "    data = np.genfromtxt(\n",
    "        path_file, delimiter=\",\", skip_header=1)\n",
    "    y = np.genfromtxt(\n",
    "        path_file, delimiter=\",\", skip_header=1, usecols=[1],dtype=str)\n",
    "    y[y=='b'] = -1\n",
    "    y[y=='s'] = 1\n",
    "    x = np.genfromtxt(\n",
    "        path_file, delimiter=\",\", skip_header=1, usecols=range(2,32))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'Data/'\n",
    "DATA_TEST = 'test.csv'\n",
    "DATA_TRAIN = 'train.csv'\n",
    "\n",
    "tx,y = load_data(DATA_FOLDER+DATA_TRAIN)\n",
    "tx_sub = np.genfromtxt(DATA_FOLDER+DATA_TEST, delimiter=\",\", skip_header=1, usecols=range(2,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noisy features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noisy_features(x,features):\n",
    "    '''removes some features (columns) of x'''\n",
    "    D = x.shape[1]\n",
    "    to_keep = [True]*D\n",
    "    for i in features:\n",
    "        to_keep[i] = False\n",
    "    return x[:,to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_ratio(df):\n",
    "    '''functions returning a pandas DataFrame, showing for each feature its percentage of outliers'''\n",
    "    nb_row = len(df)\n",
    "    res = {'var_name':[],'percentage_outliers':[]}\n",
    "    for column in df.columns[2:]:\n",
    "        nb_out = df[df[column] == -999.0][column].count()\n",
    "        res['var_name'].append(column)\n",
    "        res['percentage_outliers'].append(nb_out/nb_row*100)\n",
    "    res = pd.DataFrame(data=res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from the data all the features with a percentage of outliers greater than `seuil_outlier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil_outliers = 100\n",
    "# sort the features from the \"noisiest\" to the \"cleanest\"\n",
    "data = pd.read_csv(DATA_FOLDER+DATA_TRAIN)\n",
    "train_out = outliers_ratio(data).sort_values(ascending=False, by='percentage_outliers')\n",
    "# features to remove\n",
    "features = train_out[train_out.percentage_outliers >= seuil_outliers].index\n",
    "tx,tx_sub = remove_noisy_features(tx,features),remove_noisy_features(tx_sub,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio=0.8, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "        \n",
    "    Returns:\n",
    "        x_tr: numpy array containing the train data.\n",
    "        x_te: numpy array containing the test data.\n",
    "        y_tr: numpy array containing the train labels.\n",
    "        y_te: numpy array containing the test labels.\n",
    "        \n",
    "    >>> split_data(np.arange(13), np.arange(13), 0.8, 1)\n",
    "    (array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]), array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]))\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    cut = int(N*ratio)\n",
    "    permutation = np.random.permutation(N)\n",
    "    x,y = x[permutation],y[permutation]\n",
    "    return x[:cut],x[cut:],y[:cut],y[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "tx_tr,tx_te,y_tr,y_te = split_data(tx,y,ratio=split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 30), (50000,))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_te.shape,y_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data and manage outliers\n",
    "We set the outliers to the mean value of the feature (calculated without the outliers), so that they are set to 0 when the standardization is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_manage_outliers(x):\n",
    "    \"\"\"\n",
    "    set outliers to the mean value of the feature, then standardize the data\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of shape=(N,D)\n",
    "    Returns:\n",
    "        A numpy array of shape (N,D) with outliers set to the mean value of the features (axis 0)\n",
    "    \"\"\"\n",
    "    for j in range(x.shape[1]):\n",
    "        mean = np.mean(x[:,j][x[:,j]!=-999.0])\n",
    "        std = np.std(x[:,j][x[:,j]!=-999.0])\n",
    "        x[:,j][x[:,j]==-999.0] = mean\n",
    "        x[:,j] = (x[:,j]-mean)/std\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_tr = standardize_manage_outliers(tx_tr)\n",
    "tx_te = standardize_manage_outliers(tx_te)\n",
    "tx_sub = standardize_manage_outliers(tx_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 5\n",
    "tx_tr_aug = build_poly(tx_tr,degree=deg)\n",
    "tx_te_aug = build_poly(tx_te,degree=deg)\n",
    "tx_sub_aug = build_poly(tx_sub,degree=deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_fct = [i for i in range(6)]\n",
    "fct1 = lambda x:np.log(np.abs(x+1e-4))*x\n",
    "fct2 = lambda x:np.cos(x)/(np.abs(x)+1e-4)\n",
    "fct2_1 = lambda x:np.cos(x*2)/(np.abs(2*x)+1e-4)\n",
    "fct3 = lambda x:np.sin(x)\n",
    "fct3_1 = lambda x:np.sin(x*2)\n",
    "fct4 = lambda x: np.sin(np.exp(x))\n",
    "fct5 = lambda x:np.sinc(x)\n",
    "fct6 = lambda x:np.cos(x)/(1+np.exp(x))\n",
    "fct7 = lambda x:np.sin(x)/(1+np.exp(x))\n",
    "for fct in [fct2, fct2_1, fct3, fct3_1, fct5, fct6]:\n",
    "    tx_tr_aug = add_fct(tx_tr, tx_tr_aug, features=features_to_fct, fct=fct)\n",
    "    tx_te_aug = add_fct(tx_te, tx_te_aug, features=features_to_fct, fct=fct)\n",
    "    tx_sub_aug = add_fct(tx_sub, tx_sub_aug, features=features_to_fct, fct=fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(y,seuil=0):\n",
    "    \"\"\"\n",
    "    projects y on {-1,1}\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape=(N,). Predictions of the model.\n",
    "        seuil: float. Threshold for the projection.\n",
    "    Returns:\n",
    "        y: projection of the input y on {-1,1} according to the threshold\n",
    "    \"\"\"\n",
    "    y[y<seuil] = -1\n",
    "    y[y>=seuil] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,loss = ridge_regression(y_tr,tx_tr_aug,lambda_=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model(w,x_test,y_test,seuil=0):\n",
    "    '''\n",
    "    Function to test the accuracy of the model\n",
    "    \n",
    "    Args:\n",
    "        w: numpy array of shape=(D,). Weights of the model.\n",
    "        x_test: numpy array of shape=(N,D). Test data.\n",
    "        y_test: numpy array of shape=(N,). Test labels.\n",
    "    Returns:\n",
    "        y: numpy array of shape=(N,). Projection of y on {-1,1} (see the classify function)\n",
    "        accuracy: float. Number of good predicted labels divided by total number of prediction (N).\n",
    "    '''\n",
    "    y = classify(np.dot(x_test,w),seuil)\n",
    "    diff= (y_test == classify(y))\n",
    "    accuracy = diff.sum()/len(diff)\n",
    "    return y,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  1., -1., ..., -1.,  1., -1.]), 0.82142)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_model(w,tx_te_aug,y_te,seuil=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-221-13243293827e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlambda_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtx_tr_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtx_te_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\EPFL\\Cours\\Machine Learning\\ML_course\\projects\\project1\\implementations_bis.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "degrees = [1,2,3,5,8,12]\n",
    "lambdas = np.logspace(-3,1,num=6)\n",
    "accuracies = np.zeros((len(degrees),len(lambdas)))\n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    deg = degrees[i]\n",
    "    for j in range(len(lambdas)):\n",
    "        lambda_ = lambdas[j]\n",
    "        tx_tr_aug = build_poly(tx_tr,degree=deg)\n",
    "        tx_te_aug = build_poly(tx_te,degree=deg)\n",
    "        try:\n",
    "            w,loss = ridge_regression(y_tr,tx_tr_aug,lambda_=lambda_)\n",
    "            accuracies[i,j] = check_model(w,tx_te_aug,y_te)[1]\n",
    "        except Exception as e:\n",
    "            accuracies[i,j] = -1\n",
    "    print('{i}/{n}'.format(i=i,n=len(degrees)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal param: lambda = 0.00630957344480193, degree = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7957733333333333"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.unravel_index(np.argmax(accuracies, axis=None), accuracies.shape)\n",
    "accuracies[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.00630957344480193, degree = 5\n"
     ]
    }
   ],
   "source": [
    "print('lambda = {lambda_}, degree = {deg}'.format(deg = degrees[ind[0]],lambda_=lambdas[ind[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_pred(y):\n",
    "    '''\n",
    "    Saves the predicted labels y in the appropriate format.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape=(N,). Predicted labels.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    ids = np.genfromtxt(DATA_FOLDER+DATA_TEST, delimiter=\",\", skip_header=1,usecols=[0])\n",
    "    to_submit = np.zeros((y_sub.shape[0],2))\n",
    "    to_submit[:,0] = ids\n",
    "    to_submit[:,1] = y\n",
    "    np.savetxt('submission.csv', to_submit,delimiter=',',header='Id,Prediction',comments='')\n",
    "    print('Successfully saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sub = classify(np.dot(tx_sub_aug,w))\n",
    "y_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29279280864708096"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_sub[y_sub==1])/len(y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved\n"
     ]
    }
   ],
   "source": [
    "submit_pred(y_sub)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
